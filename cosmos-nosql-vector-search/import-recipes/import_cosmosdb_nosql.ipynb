{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populate Cosmos DB for NoSQL from csv file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating database and container..\n",
      "Initializing Azure OpenAI client..\n",
      "Loading CSV file..\n",
      "Total number of records in the CSV file: 2231142\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json, os, uuid\n",
    "from openai import AzureOpenAI\n",
    "from azure.cosmos import CosmosClient, exceptions, PartitionKey\n",
    "from azure.cosmos.cosmos_client import ThroughputProperties\n",
    "from azure.cosmos.partition_key import PartitionKey\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt  \n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(10))\n",
    "def generate_embeddings(openai_client, text):\n",
    "    \"\"\"\n",
    "    Generates embeddings for a given text using the OpenAI API v1.x\n",
    "    \"\"\"\n",
    "    \n",
    "    return openai_client.embeddings.create(\n",
    "        input = text,\n",
    "        model= os.getenv(\"AZURE_OPENAI_EMBEDDING_MODEL\")\n",
    "    ).data[0].embedding\n",
    "\n",
    "# Step 1: Configure your Cosmos DB connection\n",
    "COSMOS_DB_ENDPOINT = os.getenv('AZURE_COSMOSDB_NOSQL_ENDPOINT')\n",
    "COSMOS_DB_KEY = os.getenv('AZURE_COSMOSDB_NOSQL_KEY')\n",
    "DATABASE_NAME = os.getenv('AZURE_COSMOSDB_NOSQL_DATABASE')\n",
    "CONTAINER_NAME = os.getenv('AZURE_COSMOSDB_NOSQL_CONTAINER')\n",
    "PARTITION_KEY = os.getenv(\"AZURE_COSMOSDB_NOSQL_CONTAINER_PARTITION_KEY_PATH\")\n",
    "OFFER_THROUGHPUT = os.getenv(\"AZURE_COSMOSDB_NOSQL_THROUGHPUT\")\n",
    "CSV_FILE_PATH = \"recipes_data.csv\"\n",
    "\n",
    "throughput_properties = ThroughputProperties(auto_scale_max_throughput=OFFER_THROUGHPUT)\n",
    "\n",
    "indexing_policy = {\n",
    "    \"includedPaths\": [\n",
    "        {\"path\": \"/*\"},\n",
    "    ],\n",
    "    \"excludedPaths\": [\n",
    "        {\"path\": \"/\\\"_etag\\\"/?\"},\n",
    "        {\"path\": \"/embedding/*\"}\n",
    "    ],\n",
    "    \"vectorIndexes\": [\n",
    "        {\n",
    "            \"path\": \"/embedding\",\n",
    "            \"type\": \"quantizedFlat\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "vector_embedding_policy = {\n",
    "    \"vectorEmbeddings\": [        \n",
    "        {\n",
    "            \"path\": \"/embedding\",\n",
    "            \"dataType\": \"float32\",\n",
    "            \"distanceFunction\": \"cosine\",\n",
    "            \"dimensions\": 384\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "# Step 2: Initialize Cosmos DB client\n",
    "print(\"Creating database and container..\")\n",
    "client = CosmosClient(COSMOS_DB_ENDPOINT, COSMOS_DB_KEY)\n",
    "database = client.create_database_if_not_exists(id=DATABASE_NAME)\n",
    "container = database.create_container_if_not_exists(\n",
    "    id=CONTAINER_NAME,\n",
    "    partition_key=PartitionKey(path=PARTITION_KEY),\n",
    "    indexing_policy=indexing_policy,\n",
    "    vector_embedding_policy=vector_embedding_policy,\n",
    "    offer_throughput=throughput_properties\n",
    ")\n",
    "\n",
    "# Step 3: Initialize Azure OpenAI client\n",
    "print(\"Initializing Azure OpenAI client..\")\n",
    "openai_client = AzureOpenAI(\n",
    "    api_key = os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "    api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\"),  \n",
    "    azure_endpoint =os.getenv(\"AZURE_OPENAI_ENDPOINT\") \n",
    ")\n",
    "\n",
    "# Step 4: Load CSV file using pandas\n",
    "print(\"Loading CSV file..\")\n",
    "df = pd.read_csv(CSV_FILE_PATH)\n",
    "\n",
    "df_length = len(df)\n",
    "print(f\"Total number of records in the CSV file: {df_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing content to Cosmos DB..\n",
      "Rows loaded: 1000.\n",
      "Rows loaded: 2000.\n",
      "Rows loaded: 3000.\n",
      "Rows loaded: 4000.\n",
      "Rows loaded: 5000.\n",
      "Rows loaded: 6000.\n",
      "Rows loaded: 7000.\n",
      "Rows loaded: 8000.\n",
      "Rows loaded: 9000.\n",
      "Rows loaded: 10000.\n",
      "Rows loaded: 11000.\n",
      "Rows loaded: 12000.\n",
      "Rows loaded: 13000.\n",
      "Rows loaded: 14000.\n",
      "Rows loaded: 15000.\n",
      "Rows loaded: 16000.\n",
      "Rows loaded: 17000.\n",
      "Rows loaded: 18000.\n",
      "Rows loaded: 19000.\n",
      "Rows loaded: 20000.\n",
      "Rows loaded: 21000.\n",
      "Rows loaded: 22000.\n",
      "Rows loaded: 23000.\n",
      "Rows loaded: 24000.\n",
      "Rows loaded: 25000.\n",
      "Rows loaded: 26000.\n",
      "Rows loaded: 27000.\n",
      "Rows loaded: 28000.\n",
      "Rows loaded: 29000.\n",
      "Rows loaded: 30000.\n",
      "Rows loaded: 31000.\n",
      "Rows loaded: 32000.\n",
      "Rows loaded: 33000.\n",
      "Rows loaded: 34000.\n",
      "Rows loaded: 35000.\n",
      "Rows loaded: 36000.\n",
      "Rows loaded: 37000.\n",
      "Rows loaded: 38000.\n",
      "Rows loaded: 39000.\n",
      "Rows loaded: 40000.\n",
      "Rows loaded: 41000.\n",
      "Rows loaded: 42000.\n",
      "Rows loaded: 43000.\n",
      "Rows loaded: 44000.\n",
      "Rows loaded: 45000.\n",
      "Rows loaded: 46000.\n",
      "Rows loaded: 47000.\n",
      "Rows loaded: 48000.\n",
      "Rows loaded: 49000.\n",
      "Rows loaded: 50000.\n",
      "Rows loaded: 51000.\n",
      "Rows loaded: 52000.\n",
      "Rows loaded: 53000.\n",
      "Rows loaded: 54000.\n",
      "Rows loaded: 55000.\n",
      "Rows loaded: 56000.\n",
      "Rows loaded: 57000.\n",
      "Rows loaded: 58000.\n",
      "Rows loaded: 59000.\n",
      "Rows loaded: 60000.\n",
      "Rows loaded: 61000.\n",
      "Rows loaded: 62000.\n",
      "Rows loaded: 63000.\n",
      "Rows loaded: 64000.\n",
      "Rows loaded: 65000.\n",
      "Rows loaded: 66000.\n",
      "Rows loaded: 67000.\n",
      "Rows loaded: 68000.\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Write to Cosmos DB in batches using parallel execution\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "print(\"Writing content to Cosmos DB..\")\n",
    "batch_size = 1000  # Define the batch size based on your requirements\n",
    "\n",
    "def insert_document(row):\n",
    "    try:\n",
    "        key = str(uuid.uuid4())\n",
    "        doc = {\n",
    "            \"id\": key,\n",
    "            \"title\": row['title'],\n",
    "            \"ingredients\": row['ingredients'],\n",
    "            \"directions\": row['directions'],\n",
    "            \"link\": row['link'],\n",
    "            \"source\": row['source'],\n",
    "            \"NER\": row['NER'],        \n",
    "            \"embedding\": generate_embeddings(openai_client, str(json.dumps(row.to_dict())))\n",
    "        }\n",
    "        container.create_item(doc)\n",
    "        return 1  # Return 1 to count this row as successfully inserted\n",
    "    except exceptions.CosmosHttpResponseError as e:\n",
    "        print(f'An error occurred: {e.message}')\n",
    "        return 0  # Return 0 on failure\n",
    "\n",
    "# Use ThreadPoolExecutor to insert documents in parallel\n",
    "with ThreadPoolExecutor(max_workers=os.cpu_count() * 5) as executor:\n",
    "    total_rows_loaded = 0  # Initialize counter for loaded rows\n",
    "    for start in range(0, len(df), batch_size):\n",
    "        batch_df = df.iloc[start:start + batch_size]\n",
    "        futures = [executor.submit(insert_document, row) for index, row in batch_df.iterrows()]\n",
    "        # Wait for all futures in the current batch and count successes\n",
    "        batch_rows_loaded = sum(f.result() for f in futures)\n",
    "        total_rows_loaded += batch_rows_loaded\n",
    "        print(f'Rows loaded: {total_rows_loaded}.')\n",
    "\n",
    "print(f'Data loading completed. Total rows loaded: {total_rows_loaded}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
